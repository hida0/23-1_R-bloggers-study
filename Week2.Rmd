---
title: "Week2_Tree Based Methods: Exploring the Forest"
author: "김다영"
output: html_document
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(message=F, warning=F)
```
https://www.r-bloggers.com/2022/11/tree-based-methods-exploring-the-forest/

## Introduction
본 기사는 트리를 기반으로 하는 다양한 방법들을 소개하고 있다. 이를 위해 glass 데이터를 사용하는데, 이 데이터는 유리의 유형을 예측하기 위해 사용되는 9가지 특징을 갖는 총 214개의 관측치로 이루어져 있다. `farff` 패키지를 이용하여 사용할 데이터를 다운받아 `str()` 함수로 각 변수를 확인하고, 훈련 데이터셋과 테스트 데이터셋으로 분할한다.
```{r}
library(tidyverse)
download.file("https://www.openml.org/data/download/41/dataset_41_glass.arff", "data.arff")
df=farff::readARFF("data.arff")
```
```{r}
str(df)
```
```{r}
df=droplevels(df) #사용되지 않는 레벨을 제거, 7에서 6으로 감소
set.seed(1234) #동일한 분할 결과
test=sample(1:nrow(df), floor(nrow(df)*0.2)) #214개 중에 42개 랜덤 추출
testdf=df[test,]
traindf=df[-test,]
```

## Decision Tree
트리 기반의 모델 중에 가장 기본적인 모델은 decision tree이다. 이 모델에서는 먼저 모든 클래스가 트리의 루트에 있는 컨테이너에 저장된다. 루트는 특정 피처 값에 의하여 잎이라고 불리는 더 작은 2개의 컨테이너로 분할된다. 이 초기의 두 잎 이후에 추가적으로 4개의 잎으로 더 분할하며, 이때 새 잎의 반응 값을 잘 정렬해야 한다. 원하는 결과를 얻을 때까지 새로운 잎으로 나누고 생성하는 과정을 계속한다.

Decision tree의 가장 큰 단점은 과적합되기 매우 쉽다는 것이다. 매우 많은 잎으로 갈라진 나무는 훈련셋에서는 100%의 정확도를 갖지만 테스트셋에서는 잘 적용되지 않는다. 따라서 일정한 수로 잎의 수를 잘라내거나 가지치기 하여 과적합을 방지해야 한다.
`tree` 패지키를 사용하여 decision tree를 구현할 수 있다. 훈련 데이터셋을 이용하여 변수 Type에 대한 decision tree를 구현한다.
```{r}
mdltree=tree::tree(Type~., data=traindf)
```

`plot()` 함수로 트리를 쉽게 시각화할 수 있으며, 또 `text()` 함수를 이용하여 각각의 잎이 갈라지는 구간에 그 조건을 보여주는 텍스트를 추가할 수 있다.
```{r}
mdltree %>%
plot(mdltree)
text(mdltree, cex=0.6) #글씨 크기 조절
```

`predict()` 함수와 테스트 데이터셋을 이용하여 decision tree의 성능을 테스트한다. 반응 변수가 factor형이므로, 예측된 결과는 모든 관측치가 각각의 요인에 할당된 확률을 갖는 행렬을 반환한다.
```{r}
treevalues=predict(mdltree, newdata=testdf)
head(treevalues)
```
`colnames()`는 모든 열의 이름을, `max.col()` 함수는 행렬의 열 중에서 가장 큰 값을 갖는 열의 숫자를 반환한다. 두 함수를 이용하여 각각의 관측치의 예측 factor를 반환한다.
```{r}
treepred=colnames(treevalues)[max.col(treevalues)]
head(treepred)
```
예측된 factor와 실제 테스트 셋의 Type이 일치하는지 확인하여 평균값을 decision tree의 정확도로 측정한다.
```{r}
Acctree=mean(treepred==testdf$Type)
Acctree
```
예측 결과 정확도가 0.5로 높지 않았음을 알 수 있다. 이후 더 높은 수준의 정확도를 얻을 수 있는 몇 가지 방법을 소개하고 있다.

## Bagging
Bootstrapping은 제한된 관측치를 가질 때 유용한 방법이다. 일반적으로 우리는 어떤 가방에서 훈련 관측치를 꺼내는 방식으로 모델을 훈련시킨다. Bootstrapping은 이 관측치를 이용하여 모델을 훈련시킨 후, 다시 가방에 넣고 관측치를 다시 선택하여 꺼내는 것을 반복하는 과정을 의미한다.

Bagging은 Bootstrapping Aggregation의 약자인데 Bootstrapping에서 한단계 더 나아간 과정으로, Bootstrapping을 여러번 해서 부트스트랩된 데이터셋을 만들고, 각 세트로 모델을 훈련하여 결과 값을 평균화하여 최적화된 모델을 생성한다. 훈련 관측치들은 랜덤하게 선택되므로 각각의 부트스트랩 모델은 독립적이다.

Bagging은 `randomForest()` 함수를 이용하여 구현할 수 있다. 기본적으로 피처들의 부분집합으로 트리를 생성하지만, 모든 피처를 사용할 수도 있다. 기본 방식은 생성된 트리에 대하여 과반수 투표를 하거나 예측을 평균화하는 것이다.

```{r}
library(randomForest)
mdlbag=randomForest(Type~., data=traindf, mtry=ncol(df)-1, ntree=500) #변수 9개
#mtry: 나무에서 분할할 때 랜덤하게 표본 추출되는 변수의 수, 기본 값은 sqrt(변수의 수)
#ntree: 증가시킬 수 있는 가지의 수, 모델에서 만들 의사결정 나무의 개수
```

`predict()` 함수와 테스트 데이터셋을 이용하여 random forest의 성능을 테스트한다. 
```{r}
bagpreds=predict(mdlbag, newdata=testdf)
head(bagpreds)
```
예측된 factor와 실제 테스트 셋의 Type이 일치하는지 확인하여 평균값으로 정확도를 측정한다.
```{r}
Accbag=mean(bagpreds==testdf$Type)
Accbag
```

## Random Forest
Random forest는 Bagging의 한 종류로, 여러 개의 decision tree를 함께 결합하여 예측이 생성되는 앙상블 모델이다. Random forest는 각각의 트리를 만들 때 임의의 피처 집합을 선택하는데, 이로부터 트리는 더 고유해지고 다양성을 갖게 되어 모형 간의 상관관계를 줄일 수 있으며 bagging과의 차이점이 된다. 생성한 여러 개의 트리로부터의 결과 값을 평균화하거나 다수결의 원칙을 이용하여 최종 결과를 예측하여 최적의 모델을 생성한다.

마찬가지로 `randomForest()` 함수를 이용하여 구현할 수 있으며, `predict()` 함수와 테스트 데이터셋을 이용하여 예측한다.
```{r}
mdlrf=randomForest(Type~., data=traindf, ntree=500) #변수3개
rfpreds=predict(mdlrf, newdata=testdf)
head(rfpreds)
```
예측된 factor와 실제 테스트 셋의 Type이 일치하는지 확인하여 random forest의 정확도로 측정한다.
```{r}
Accrf=mean(rfpreds==testdf$Type)
Accrf
```

## Boosting
Boosting은 bagging과 유사하게 다중의 모델을 사용하는 기법으로, 여러 트리를 만들기 위해 순차적으로 트리를 생성한다. 먼저 훈련 데이터셋의 일부를 이용하여 트리를 생성하고 정확도를 측정한다. 이때 트리는 잘못 분류된 데이터, 즉 잔차에 주목하여 잔차를 줄이는 방향으로 나아간다. 이 잔차 데이터에 가중치를 부여하므로 다음으로 뽑히는 데이터셋에 가중치가 부여된 데이터가 들어올 확률이 높고, 이 데이터들로 트리를 만들어 학습한다. 이렇게 각 트리는 이전 트리의 잔차를 이용하여 천천히 학습하므로 학습 속도가 느리다. 

Boosting을 구현하기 위해 `caret`과 `gbm` 패키지를 사용한다. `gbm` 패키지는 실제 모델을 제공한다. `caret` 패키지가 제공하는 `train()` 함수를 이용한다. 이 함수는 여러 분류 또는 회귀 모델을 수용하여 자동적으로 파라미터를 튜닝하고 각 모델을 적합시킨다. 
```{r}
library(caret)
library(gbm)
mdlboost=caret::train(Type~., data=traindf, method='gbm', verbose=FALSE)
#method: 사용할 분류 또는 회귀 모델을 입력
```
`predict()` 함수와 테스트 데이터셋을 이용하여 예측한다.
```{r}
boostpreds=predict(mdlboost, newdata=testdf)
head(boostpreds)
```
예측된 factor와 실제 테스트 셋의 Type이 일치하는지 확인하여 정확도를 측정한다.
```{r}
Accboost=mean(boostpreds==testdf$Type)
Accboost
```

## Bayesian Additive Regression
Bayesian Additive Regression Trees, 즉 BART는 이전 트리에서 캡쳐되지 않은 신호를 모델링하는 일부 랜덤 요소로 트리를 생성한다. 
```{r, out.width='40%'}
knitr::include_graphics("C:/Users/ekdud/Desktop/bart.png")
```

첫번째 반복에서 모든 트리는 동일한 노드로 초기화된다. 다른 반복 후에 각각의 k번째 트리를 하나씩 업데이트한다. b번째 반복동안 부분 잔차를 생성하기 위해 각 반응변수에서 예측 값을 뺀다. BART는 이렇게 구한 잔차에 새로운 트리를 적합시키는 대신에 임의로 이전의 반복에서 트리를 선택하여 적합성을 높이는 트리를 선택한다.

## Conclusion
Decision tree는 이해하기 쉬운 모델이지만 데이터가 과적합되는 경향이 있으며, 이를 방지하기 위해 가지치기를 하면 정확도가 하락한다. 본 기사에서 측정한 정확도는 50%이다.

앙상블 모델은 서로 다른 관측치를 선택하여 생성된 트리들을 평균화하는 bagging으로부터 도입된 기법이다. Random forest가 한 종류로, 피처를 선택하는데 사용되는 랜덤 요소를 증가시킴으로써 정확도를 66.67%로 향상시킨다.

Boosting으로 이전에 생성된 트리를 보완하여 생성한 모델에서 정확도가 69.05%로 가장 향상되었다.

사용하는 데이터와 목적에 따라 다르고, 비슷한 방법을 이용하여 유사한 결과를 생성하므로 어떤 모델이 가장 최고라고 말할 수는 없다. 모든 모델을 이용하여 훈련시켜서 사용하고자 하는 데이터에 대해 가장 최선의 성능을 나타내는 모델을 찾는 것이 바람직한 학습 과정이라고 생각된다.